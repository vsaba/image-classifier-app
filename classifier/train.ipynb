{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-20T23:30:58.945401708Z",
     "start_time": "2023-12-20T23:30:58.920584509Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision.datasets import MNIST\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "64e6c7c0b75de540",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-20T23:31:00.521926645Z",
     "start_time": "2023-12-20T23:31:00.345256482Z"
    }
   },
   "outputs": [],
   "source": [
    "data_transform = transforms.Compose([transforms.ToTensor(), \n",
    "                                     transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "train_data = MNIST(root=\"./data\", train=True, download=True, transform=data_transform)\n",
    "test_data = MNIST(root=\"./data\", train=False, download=True, transform=data_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5a8acf045733a78",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-20T23:31:01.460539503Z",
     "start_time": "2023-12-20T23:31:01.399921150Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data length:  60000\n",
      "Test data length:  10000\n"
     ]
    }
   ],
   "source": [
    "print(\"Training data length: \", len(train_data))\n",
    "print(\"Test data length: \", len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1e3293af80156c2a",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-20T23:31:04.079632884Z",
     "start_time": "2023-12-20T23:31:04.073127780Z"
    }
   },
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2a5166f49e61119d",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-20T23:31:41.436137644Z",
     "start_time": "2023-12-20T23:31:41.413790657Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier(\n",
      "  (conv1): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv2): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (dropout): Dropout(p=0.25, inplace=False)\n",
      "  (fc1): Linear(in_features=800, out_features=400, bias=True)\n",
      "  (fc2): Linear(in_features=400, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from model import Classifier\n",
    "\n",
    "net = Classifier()\n",
    "print(net)\n",
    "#net.load_model(\"./mnist_model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5cc5991ed695fcdc",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-20T19:55:13.755768826Z",
     "start_time": "2023-12-20T19:55:13.712305196Z"
    }
   },
   "outputs": [],
   "source": [
    "def train(num_epochs, model):\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        for i, data in enumerate(train_loader):\n",
    "            #unpack\n",
    "            inputs, labels = data\n",
    "\n",
    "            #reset the gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            #forward\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            #loss\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            #calculate backprop weights\n",
    "            loss.backward()\n",
    "\n",
    "            #apply learning\n",
    "            optimizer.step()\n",
    "\n",
    "            #add loss\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            if (i % 100) == 99:\n",
    "                mean_loss = running_loss / 100\n",
    "                print(\"Epoch \", epoch + 1, \"; Batch\", i + 1, \"; Train loss: \", mean_loss)\n",
    "                running_loss = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cab5eaa9579946dc",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-20T20:06:26.568952815Z",
     "start_time": "2023-12-20T20:06:26.523003545Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.005, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3cedf74d7a9bbb2d",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-20T20:15:52.498355481Z",
     "start_time": "2023-12-20T20:06:29.341274850Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1 ; Batch 100 ; Train loss:  0.03492684114200529\n",
      "Epoch  1 ; Batch 200 ; Train loss:  0.03570411839755252\n",
      "Epoch  1 ; Batch 300 ; Train loss:  0.038326805524993686\n",
      "Epoch  1 ; Batch 400 ; Train loss:  0.0429396550077945\n",
      "Epoch  1 ; Batch 500 ; Train loss:  0.031962334311101584\n",
      "Epoch  1 ; Batch 600 ; Train loss:  0.037949443323304875\n",
      "Epoch  1 ; Batch 700 ; Train loss:  0.03952873055357486\n",
      "Epoch  1 ; Batch 800 ; Train loss:  0.04050223772530444\n",
      "Epoch  1 ; Batch 900 ; Train loss:  0.044642827765783294\n",
      "Epoch  2 ; Batch 100 ; Train loss:  0.03288874818943441\n",
      "Epoch  2 ; Batch 200 ; Train loss:  0.04430500017013401\n",
      "Epoch  2 ; Batch 300 ; Train loss:  0.03714826587587595\n",
      "Epoch  2 ; Batch 400 ; Train loss:  0.040992767363786695\n",
      "Epoch  2 ; Batch 500 ; Train loss:  0.031777181772049516\n",
      "Epoch  2 ; Batch 600 ; Train loss:  0.037035333197563886\n",
      "Epoch  2 ; Batch 700 ; Train loss:  0.0360169585084077\n",
      "Epoch  2 ; Batch 800 ; Train loss:  0.0337024169054348\n",
      "Epoch  2 ; Batch 900 ; Train loss:  0.0415317221195437\n",
      "Epoch  3 ; Batch 100 ; Train loss:  0.041909893015399574\n",
      "Epoch  3 ; Batch 200 ; Train loss:  0.03542691190377809\n",
      "Epoch  3 ; Batch 300 ; Train loss:  0.040902209175983445\n",
      "Epoch  3 ; Batch 400 ; Train loss:  0.032165401893435044\n",
      "Epoch  3 ; Batch 500 ; Train loss:  0.03622640408459119\n",
      "Epoch  3 ; Batch 600 ; Train loss:  0.03422128063393757\n",
      "Epoch  3 ; Batch 700 ; Train loss:  0.03413293825578876\n",
      "Epoch  3 ; Batch 800 ; Train loss:  0.0346329448511824\n",
      "Epoch  3 ; Batch 900 ; Train loss:  0.030603177869925276\n",
      "Epoch  4 ; Batch 100 ; Train loss:  0.035489345499663615\n",
      "Epoch  4 ; Batch 200 ; Train loss:  0.02759414091007784\n",
      "Epoch  4 ; Batch 300 ; Train loss:  0.027040787752484904\n",
      "Epoch  4 ; Batch 400 ; Train loss:  0.03150818520050962\n",
      "Epoch  4 ; Batch 500 ; Train loss:  0.031009796888101845\n",
      "Epoch  4 ; Batch 600 ; Train loss:  0.03687402945710346\n",
      "Epoch  4 ; Batch 700 ; Train loss:  0.030286957070347852\n",
      "Epoch  4 ; Batch 800 ; Train loss:  0.03242233919911087\n",
      "Epoch  4 ; Batch 900 ; Train loss:  0.03433503404725343\n",
      "Epoch  5 ; Batch 100 ; Train loss:  0.030842667675751726\n",
      "Epoch  5 ; Batch 200 ; Train loss:  0.032094395253807306\n",
      "Epoch  5 ; Batch 300 ; Train loss:  0.03802753409021534\n",
      "Epoch  5 ; Batch 400 ; Train loss:  0.030916757686063646\n",
      "Epoch  5 ; Batch 500 ; Train loss:  0.03527959691127762\n",
      "Epoch  5 ; Batch 600 ; Train loss:  0.03621021617087536\n",
      "Epoch  5 ; Batch 700 ; Train loss:  0.030775197250768542\n",
      "Epoch  5 ; Batch 800 ; Train loss:  0.036904972371412444\n",
      "Epoch  5 ; Batch 900 ; Train loss:  0.030250242275651543\n",
      "Epoch  6 ; Batch 100 ; Train loss:  0.028371919977944346\n",
      "Epoch  6 ; Batch 200 ; Train loss:  0.02450156660634093\n",
      "Epoch  6 ; Batch 300 ; Train loss:  0.036635947573231534\n",
      "Epoch  6 ; Batch 400 ; Train loss:  0.03572798789129592\n",
      "Epoch  6 ; Batch 500 ; Train loss:  0.03021882095141336\n",
      "Epoch  6 ; Batch 600 ; Train loss:  0.027564427095931023\n",
      "Epoch  6 ; Batch 700 ; Train loss:  0.03329315191251226\n",
      "Epoch  6 ; Batch 800 ; Train loss:  0.027044341802829875\n",
      "Epoch  6 ; Batch 900 ; Train loss:  0.030513014913303778\n",
      "Epoch  7 ; Batch 100 ; Train loss:  0.03572118881274946\n",
      "Epoch  7 ; Batch 200 ; Train loss:  0.027060171808116137\n",
      "Epoch  7 ; Batch 300 ; Train loss:  0.0331225581705803\n",
      "Epoch  7 ; Batch 400 ; Train loss:  0.028059071186580697\n",
      "Epoch  7 ; Batch 500 ; Train loss:  0.03275312506419141\n",
      "Epoch  7 ; Batch 600 ; Train loss:  0.036752038998529316\n",
      "Epoch  7 ; Batch 700 ; Train loss:  0.02808279594173655\n",
      "Epoch  7 ; Batch 800 ; Train loss:  0.02937010784284212\n",
      "Epoch  7 ; Batch 900 ; Train loss:  0.026766743849148043\n",
      "Epoch  8 ; Batch 100 ; Train loss:  0.03219613060005941\n",
      "Epoch  8 ; Batch 200 ; Train loss:  0.025872600742150097\n",
      "Epoch  8 ; Batch 300 ; Train loss:  0.030168599875178188\n",
      "Epoch  8 ; Batch 400 ; Train loss:  0.023507732291473075\n",
      "Epoch  8 ; Batch 500 ; Train loss:  0.03269808425335213\n",
      "Epoch  8 ; Batch 600 ; Train loss:  0.026955882995389403\n",
      "Epoch  8 ; Batch 700 ; Train loss:  0.024558579565491526\n",
      "Epoch  8 ; Batch 800 ; Train loss:  0.032559446353698146\n",
      "Epoch  8 ; Batch 900 ; Train loss:  0.023624218034092336\n",
      "Epoch  9 ; Batch 100 ; Train loss:  0.02401342292665504\n",
      "Epoch  9 ; Batch 200 ; Train loss:  0.027432846130686812\n",
      "Epoch  9 ; Batch 300 ; Train loss:  0.02551659025833942\n",
      "Epoch  9 ; Batch 400 ; Train loss:  0.026196330036618747\n",
      "Epoch  9 ; Batch 500 ; Train loss:  0.025553185156313704\n",
      "Epoch  9 ; Batch 600 ; Train loss:  0.031947087537264454\n",
      "Epoch  9 ; Batch 700 ; Train loss:  0.027499894734064584\n",
      "Epoch  9 ; Batch 800 ; Train loss:  0.027434930801391602\n",
      "Epoch  9 ; Batch 900 ; Train loss:  0.02527732490678318\n",
      "Epoch  10 ; Batch 100 ; Train loss:  0.02631884887232445\n",
      "Epoch  10 ; Batch 200 ; Train loss:  0.023466452346765435\n",
      "Epoch  10 ; Batch 300 ; Train loss:  0.027474749262910336\n",
      "Epoch  10 ; Batch 400 ; Train loss:  0.031040216767578385\n",
      "Epoch  10 ; Batch 500 ; Train loss:  0.025318180189933627\n",
      "Epoch  10 ; Batch 600 ; Train loss:  0.0312918870249996\n",
      "Epoch  10 ; Batch 700 ; Train loss:  0.030738707518321463\n",
      "Epoch  10 ; Batch 800 ; Train loss:  0.02697076360345818\n",
      "Epoch  10 ; Batch 900 ; Train loss:  0.029298045464092867\n",
      "Epoch  11 ; Batch 100 ; Train loss:  0.027807689763139933\n",
      "Epoch  11 ; Batch 200 ; Train loss:  0.0256755938322749\n",
      "Epoch  11 ; Batch 300 ; Train loss:  0.019138311167480423\n",
      "Epoch  11 ; Batch 400 ; Train loss:  0.023147900324547662\n",
      "Epoch  11 ; Batch 500 ; Train loss:  0.026305620671482757\n",
      "Epoch  11 ; Batch 600 ; Train loss:  0.02552982513036113\n",
      "Epoch  11 ; Batch 700 ; Train loss:  0.027935415338724852\n",
      "Epoch  11 ; Batch 800 ; Train loss:  0.029059704229584895\n",
      "Epoch  11 ; Batch 900 ; Train loss:  0.03305511470942293\n",
      "Epoch  12 ; Batch 100 ; Train loss:  0.021855399328051136\n",
      "Epoch  12 ; Batch 200 ; Train loss:  0.024322551265941003\n",
      "Epoch  12 ; Batch 300 ; Train loss:  0.029299244790163357\n",
      "Epoch  12 ; Batch 400 ; Train loss:  0.029443412671098484\n",
      "Epoch  12 ; Batch 500 ; Train loss:  0.025128704183152877\n",
      "Epoch  12 ; Batch 600 ; Train loss:  0.026700885847676544\n",
      "Epoch  12 ; Batch 700 ; Train loss:  0.02655390170752071\n",
      "Epoch  12 ; Batch 800 ; Train loss:  0.02541554320952855\n",
      "Epoch  12 ; Batch 900 ; Train loss:  0.02826827247219626\n",
      "Epoch  13 ; Batch 100 ; Train loss:  0.028154943335684947\n",
      "Epoch  13 ; Batch 200 ; Train loss:  0.02217119958513649\n",
      "Epoch  13 ; Batch 300 ; Train loss:  0.022371554412529804\n",
      "Epoch  13 ; Batch 400 ; Train loss:  0.023318730593309737\n",
      "Epoch  13 ; Batch 500 ; Train loss:  0.022141318962094373\n",
      "Epoch  13 ; Batch 600 ; Train loss:  0.028778805503970942\n",
      "Epoch  13 ; Batch 700 ; Train loss:  0.02354687806451693\n",
      "Epoch  13 ; Batch 800 ; Train loss:  0.02980303002637811\n",
      "Epoch  13 ; Batch 900 ; Train loss:  0.02237895630998537\n",
      "Epoch  14 ; Batch 100 ; Train loss:  0.024844295060029255\n",
      "Epoch  14 ; Batch 200 ; Train loss:  0.02430171304382384\n",
      "Epoch  14 ; Batch 300 ; Train loss:  0.020538620539591646\n",
      "Epoch  14 ; Batch 400 ; Train loss:  0.019985275949293283\n",
      "Epoch  14 ; Batch 500 ; Train loss:  0.02433058970957063\n",
      "Epoch  14 ; Batch 600 ; Train loss:  0.025875000938540324\n",
      "Epoch  14 ; Batch 700 ; Train loss:  0.02108983365644235\n",
      "Epoch  14 ; Batch 800 ; Train loss:  0.026630156359169634\n",
      "Epoch  14 ; Batch 900 ; Train loss:  0.02545371965505183\n",
      "Epoch  15 ; Batch 100 ; Train loss:  0.02293120334390551\n",
      "Epoch  15 ; Batch 200 ; Train loss:  0.025210427807760425\n",
      "Epoch  15 ; Batch 300 ; Train loss:  0.017316353963688014\n",
      "Epoch  15 ; Batch 400 ; Train loss:  0.024950117503176442\n",
      "Epoch  15 ; Batch 500 ; Train loss:  0.024295091099338605\n",
      "Epoch  15 ; Batch 600 ; Train loss:  0.023523873816011474\n",
      "Epoch  15 ; Batch 700 ; Train loss:  0.027284432637679858\n",
      "Epoch  15 ; Batch 800 ; Train loss:  0.023550465976586565\n",
      "Epoch  15 ; Batch 900 ; Train loss:  0.02317410490504699\n",
      "Epoch  16 ; Batch 100 ; Train loss:  0.019091661128913983\n",
      "Epoch  16 ; Batch 200 ; Train loss:  0.019675019533751767\n",
      "Epoch  16 ; Batch 300 ; Train loss:  0.02293015805480536\n",
      "Epoch  16 ; Batch 400 ; Train loss:  0.025633393293246626\n",
      "Epoch  16 ; Batch 500 ; Train loss:  0.021763430637074633\n",
      "Epoch  16 ; Batch 600 ; Train loss:  0.02761560585582629\n",
      "Epoch  16 ; Batch 700 ; Train loss:  0.02282106803962961\n",
      "Epoch  16 ; Batch 800 ; Train loss:  0.02615283488892601\n",
      "Epoch  16 ; Batch 900 ; Train loss:  0.027355090524069967\n",
      "Epoch  17 ; Batch 100 ; Train loss:  0.02550660390232224\n",
      "Epoch  17 ; Batch 200 ; Train loss:  0.017521127810759936\n",
      "Epoch  17 ; Batch 300 ; Train loss:  0.026682675347547046\n",
      "Epoch  17 ; Batch 400 ; Train loss:  0.024961940429639073\n",
      "Epoch  17 ; Batch 500 ; Train loss:  0.023255766090005636\n",
      "Epoch  17 ; Batch 600 ; Train loss:  0.019563378398306668\n",
      "Epoch  17 ; Batch 700 ; Train loss:  0.023454758989973925\n",
      "Epoch  17 ; Batch 800 ; Train loss:  0.019975260856735986\n",
      "Epoch  17 ; Batch 900 ; Train loss:  0.021190217325347475\n",
      "Epoch  18 ; Batch 100 ; Train loss:  0.02059605551621644\n",
      "Epoch  18 ; Batch 200 ; Train loss:  0.022734356488363118\n",
      "Epoch  18 ; Batch 300 ; Train loss:  0.02126645757642109\n",
      "Epoch  18 ; Batch 400 ; Train loss:  0.01909762453957228\n",
      "Epoch  18 ; Batch 500 ; Train loss:  0.021355627513548824\n",
      "Epoch  18 ; Batch 600 ; Train loss:  0.018660927288001405\n",
      "Epoch  18 ; Batch 700 ; Train loss:  0.01969158560212236\n",
      "Epoch  18 ; Batch 800 ; Train loss:  0.02436556677770568\n",
      "Epoch  18 ; Batch 900 ; Train loss:  0.017422021194943228\n",
      "Epoch  19 ; Batch 100 ; Train loss:  0.018489472140499855\n",
      "Epoch  19 ; Batch 200 ; Train loss:  0.02396545309922658\n",
      "Epoch  19 ; Batch 300 ; Train loss:  0.02575516886136029\n",
      "Epoch  19 ; Batch 400 ; Train loss:  0.02283166640700074\n",
      "Epoch  19 ; Batch 500 ; Train loss:  0.016811147206462918\n",
      "Epoch  19 ; Batch 600 ; Train loss:  0.02088468450761866\n",
      "Epoch  19 ; Batch 700 ; Train loss:  0.014406606365082553\n",
      "Epoch  19 ; Batch 800 ; Train loss:  0.02487761339638382\n",
      "Epoch  19 ; Batch 900 ; Train loss:  0.023455509675841313\n",
      "Epoch  20 ; Batch 100 ; Train loss:  0.018433007609128253\n",
      "Epoch  20 ; Batch 200 ; Train loss:  0.020777962000865954\n",
      "Epoch  20 ; Batch 300 ; Train loss:  0.02307533387851436\n",
      "Epoch  20 ; Batch 400 ; Train loss:  0.0153675962696434\n",
      "Epoch  20 ; Batch 500 ; Train loss:  0.02181858071053284\n",
      "Epoch  20 ; Batch 600 ; Train loss:  0.01950327893253416\n",
      "Epoch  20 ; Batch 700 ; Train loss:  0.02342463190027047\n",
      "Epoch  20 ; Batch 800 ; Train loss:  0.020151392353291157\n",
      "Epoch  20 ; Batch 900 ; Train loss:  0.02004862175148446\n",
      "Epoch  21 ; Batch 100 ; Train loss:  0.01785887251025997\n",
      "Epoch  21 ; Batch 200 ; Train loss:  0.019691409078077413\n",
      "Epoch  21 ; Batch 300 ; Train loss:  0.017592987138195893\n",
      "Epoch  21 ; Batch 400 ; Train loss:  0.01567667734751012\n",
      "Epoch  21 ; Batch 500 ; Train loss:  0.02283508213062305\n",
      "Epoch  21 ; Batch 600 ; Train loss:  0.014808661748538725\n",
      "Epoch  21 ; Batch 700 ; Train loss:  0.016055906000547113\n",
      "Epoch  21 ; Batch 800 ; Train loss:  0.02363001978432294\n",
      "Epoch  21 ; Batch 900 ; Train loss:  0.022248811586614466\n",
      "Epoch  22 ; Batch 100 ; Train loss:  0.017411013894598\n",
      "Epoch  22 ; Batch 200 ; Train loss:  0.01894036448386032\n",
      "Epoch  22 ; Batch 300 ; Train loss:  0.017512577772140502\n",
      "Epoch  22 ; Batch 400 ; Train loss:  0.019757026771694653\n",
      "Epoch  22 ; Batch 500 ; Train loss:  0.017270714457263237\n",
      "Epoch  22 ; Batch 600 ; Train loss:  0.019453066912246866\n",
      "Epoch  22 ; Batch 700 ; Train loss:  0.020195042937120888\n",
      "Epoch  22 ; Batch 800 ; Train loss:  0.019280025186599232\n",
      "Epoch  22 ; Batch 900 ; Train loss:  0.020747507916530593\n",
      "Epoch  23 ; Batch 100 ; Train loss:  0.021434454568079672\n",
      "Epoch  23 ; Batch 200 ; Train loss:  0.01711185026593739\n",
      "Epoch  23 ; Batch 300 ; Train loss:  0.017422179034329018\n",
      "Epoch  23 ; Batch 400 ; Train loss:  0.01965143104287563\n",
      "Epoch  23 ; Batch 500 ; Train loss:  0.022169988280511463\n",
      "Epoch  23 ; Batch 600 ; Train loss:  0.02036045699758688\n",
      "Epoch  23 ; Batch 700 ; Train loss:  0.014598551070957911\n",
      "Epoch  23 ; Batch 800 ; Train loss:  0.017300820033997296\n",
      "Epoch  23 ; Batch 900 ; Train loss:  0.01796449685716652\n",
      "Epoch  24 ; Batch 100 ; Train loss:  0.02133112113457173\n",
      "Epoch  24 ; Batch 200 ; Train loss:  0.01538185218058061\n",
      "Epoch  24 ; Batch 300 ; Train loss:  0.013356827531315502\n",
      "Epoch  24 ; Batch 400 ; Train loss:  0.017280052867135964\n",
      "Epoch  24 ; Batch 500 ; Train loss:  0.018539888566883746\n",
      "Epoch  24 ; Batch 600 ; Train loss:  0.01756489024293842\n",
      "Epoch  24 ; Batch 700 ; Train loss:  0.021886985657911282\n",
      "Epoch  24 ; Batch 800 ; Train loss:  0.014051340770092792\n",
      "Epoch  24 ; Batch 900 ; Train loss:  0.01610768108832417\n",
      "Epoch  25 ; Batch 100 ; Train loss:  0.015348581310245208\n",
      "Epoch  25 ; Batch 200 ; Train loss:  0.01942413580487482\n",
      "Epoch  25 ; Batch 300 ; Train loss:  0.022007670210732612\n",
      "Epoch  25 ; Batch 400 ; Train loss:  0.016999677282001358\n",
      "Epoch  25 ; Batch 500 ; Train loss:  0.019258008869655896\n",
      "Epoch  25 ; Batch 600 ; Train loss:  0.021981247996445744\n",
      "Epoch  25 ; Batch 700 ; Train loss:  0.017432564647169784\n",
      "Epoch  25 ; Batch 800 ; Train loss:  0.017938203023950338\n",
      "Epoch  25 ; Batch 900 ; Train loss:  0.014326619627536275\n",
      "Epoch  26 ; Batch 100 ; Train loss:  0.016607594004599376\n",
      "Epoch  26 ; Batch 200 ; Train loss:  0.015377167865808587\n",
      "Epoch  26 ; Batch 300 ; Train loss:  0.017030695848225152\n",
      "Epoch  26 ; Batch 400 ; Train loss:  0.011305892665113788\n",
      "Epoch  26 ; Batch 500 ; Train loss:  0.020122414414654486\n",
      "Epoch  26 ; Batch 600 ; Train loss:  0.020483950198831737\n",
      "Epoch  26 ; Batch 700 ; Train loss:  0.022358021728286985\n",
      "Epoch  26 ; Batch 800 ; Train loss:  0.015483591278461972\n",
      "Epoch  26 ; Batch 900 ; Train loss:  0.015630685594223905\n",
      "Epoch  27 ; Batch 100 ; Train loss:  0.015233228166616755\n",
      "Epoch  27 ; Batch 200 ; Train loss:  0.015865926065016536\n",
      "Epoch  27 ; Batch 300 ; Train loss:  0.014334050991456024\n",
      "Epoch  27 ; Batch 400 ; Train loss:  0.015846296419622378\n",
      "Epoch  27 ; Batch 500 ; Train loss:  0.019513821313885273\n",
      "Epoch  27 ; Batch 600 ; Train loss:  0.020579794939694692\n",
      "Epoch  27 ; Batch 700 ; Train loss:  0.020447716966737063\n",
      "Epoch  27 ; Batch 800 ; Train loss:  0.015346477661514655\n",
      "Epoch  27 ; Batch 900 ; Train loss:  0.020384719608409797\n",
      "Epoch  28 ; Batch 100 ; Train loss:  0.015938619970402215\n",
      "Epoch  28 ; Batch 200 ; Train loss:  0.02026943740027491\n",
      "Epoch  28 ; Batch 300 ; Train loss:  0.0180045063587022\n",
      "Epoch  28 ; Batch 400 ; Train loss:  0.017562948305276223\n",
      "Epoch  28 ; Batch 500 ; Train loss:  0.016870848484904853\n",
      "Epoch  28 ; Batch 600 ; Train loss:  0.017678653706389012\n",
      "Epoch  28 ; Batch 700 ; Train loss:  0.020562612610665384\n",
      "Epoch  28 ; Batch 800 ; Train loss:  0.016544456210394857\n",
      "Epoch  28 ; Batch 900 ; Train loss:  0.015480193594703451\n",
      "Epoch  29 ; Batch 100 ; Train loss:  0.01322256429848494\n",
      "Epoch  29 ; Batch 200 ; Train loss:  0.012732875983929262\n",
      "Epoch  29 ; Batch 300 ; Train loss:  0.019428164123528404\n",
      "Epoch  29 ; Batch 400 ; Train loss:  0.018620631886005868\n",
      "Epoch  29 ; Batch 500 ; Train loss:  0.020049423029850005\n",
      "Epoch  29 ; Batch 600 ; Train loss:  0.017603131021896843\n",
      "Epoch  29 ; Batch 700 ; Train loss:  0.018016820047050713\n",
      "Epoch  29 ; Batch 800 ; Train loss:  0.021787138693034648\n",
      "Epoch  29 ; Batch 900 ; Train loss:  0.015203961229417473\n",
      "Epoch  30 ; Batch 100 ; Train loss:  0.018033065647177864\n",
      "Epoch  30 ; Batch 200 ; Train loss:  0.019788680513447617\n",
      "Epoch  30 ; Batch 300 ; Train loss:  0.011587063573970227\n",
      "Epoch  30 ; Batch 400 ; Train loss:  0.01129905421854346\n",
      "Epoch  30 ; Batch 500 ; Train loss:  0.01852166682947427\n",
      "Epoch  30 ; Batch 600 ; Train loss:  0.017510683156433515\n",
      "Epoch  30 ; Batch 700 ; Train loss:  0.020278361410528304\n",
      "Epoch  30 ; Batch 800 ; Train loss:  0.01619748595636338\n",
      "Epoch  30 ; Batch 900 ; Train loss:  0.016023051757365465\n",
      "Finished training!\n"
     ]
    }
   ],
   "source": [
    "epochs = 30\n",
    "\n",
    "train(epochs, net)\n",
    "print(\"Finished training!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9eb50bed-db23-4105-8d3d-66d8bcdc61ef",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-20T20:17:34.238596446Z",
     "start_time": "2023-12-20T20:17:34.197665527Z"
    }
   },
   "outputs": [],
   "source": [
    "torch.save(net.state_dict(), \"./mnist_model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 28, 28])\n",
      "Predicted: 2 , True: 2\n"
     ]
    }
   ],
   "source": [
    "## testing the model by hand\n",
    "\n",
    "net.eval()\n",
    "image, label = next(iter(test_loader))\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "num = np.random.randint(batch_size)\n",
    "x = torch.unsqueeze(image[num], 0)\n",
    "print(x.shape)\n",
    "with torch.no_grad():\n",
    "    print(\"Predicted:\", torch.argmax(net(x)).item(), \", True:\", label[num].item())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-20T23:34:58.336309995Z",
     "start_time": "2023-12-20T23:34:58.309022495Z"
    }
   },
   "id": "cd4efbef29ca30b6",
   "execution_count": 21
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def test_model(test_dataloader, model):\n",
    "    correct_pred = 0\n",
    "    model.eval()\n",
    "\n",
    "    for images, labels in test_dataloader:\n",
    "        for i in range(len(labels)):\n",
    "            image = torch.unsqueeze(images[i], 0)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                output = model(image)\n",
    "\n",
    "            label_predicted = torch.argmax(output).item()\n",
    "            label_true = labels[i].item()\n",
    "\n",
    "            if label_predicted == label_true:\n",
    "                correct_pred += 1\n",
    "\n",
    "    total = len(test_data)\n",
    "\n",
    "    return correct_pred / total\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-20T23:35:07.861526086Z",
     "start_time": "2023-12-20T23:35:07.851797082Z"
    }
   },
   "id": "422a27e825e8fc43",
   "execution_count": 22
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of the model on the test dataset: 99.39%\n"
     ]
    }
   ],
   "source": [
    "accuracy = test_model(test_loader, net)\n",
    "\n",
    "print(f\"The accuracy of the model on the test dataset: {accuracy * 100}%\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-20T23:35:15.266349824Z",
     "start_time": "2023-12-20T23:35:10.870566110Z"
    }
   },
   "id": "986d51820758b0ac",
   "execution_count": 23
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
