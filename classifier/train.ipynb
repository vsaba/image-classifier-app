{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-26T10:19:52.548634518Z",
     "start_time": "2023-12-26T10:19:50.170997617Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision.datasets import MNIST\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "64e6c7c0b75de540",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-26T10:19:54.177751564Z",
     "start_time": "2023-12-26T10:19:54.041397818Z"
    }
   },
   "outputs": [],
   "source": [
    "data_transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                     transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "train_data = MNIST(root=\"./data\", train=True, download=True, transform=data_transform)\n",
    "test_data = MNIST(root=\"./data\", train=False, download=True, transform=data_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a8acf045733a78",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-26T10:20:13.195253466Z",
     "start_time": "2023-12-26T10:20:13.176995317Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data length:  60000\n",
      "Test data length:  10000\n"
     ]
    }
   ],
   "source": [
    "print(\"Training data length: \", len(train_data))\n",
    "print(\"Test data length: \", len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1e3293af80156c2a",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-26T10:20:15.324006941Z",
     "start_time": "2023-12-26T10:20:15.298544075Z"
    }
   },
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2a5166f49e61119d",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-26T10:20:19.294958151Z",
     "start_time": "2023-12-26T10:20:19.253095834Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier(\n",
      "  (conv1): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv2): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (dropout): Dropout(p=0.25, inplace=False)\n",
      "  (fc1): Linear(in_features=800, out_features=400, bias=True)\n",
      "  (fc2): Linear(in_features=400, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from model import Classifier\n",
    "\n",
    "net = Classifier()\n",
    "print(net)\n",
    "#net.load_model(\"./mnist_model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5cc5991ed695fcdc",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-26T10:20:22.722576053Z",
     "start_time": "2023-12-26T10:20:22.675130088Z"
    }
   },
   "outputs": [],
   "source": [
    "def train(num_epochs, model):\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        for i, data in enumerate(train_loader):\n",
    "            #unpack\n",
    "            inputs, labels = data\n",
    "\n",
    "            #reset the gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            #forward\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            #loss\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            #calculate backprop weights\n",
    "            loss.backward()\n",
    "\n",
    "            #apply learning\n",
    "            optimizer.step()\n",
    "\n",
    "            #add loss\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            if (i % 100) == 99:\n",
    "                mean_loss = running_loss / 100\n",
    "                print(\"Epoch \", epoch + 1, \"; Batch\", i + 1, \"; Train loss: \", mean_loss)\n",
    "                running_loss = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cab5eaa9579946dc",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-26T10:20:24.998201347Z",
     "start_time": "2023-12-26T10:20:24.989185263Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.005, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3cedf74d7a9bbb2d",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-26T10:32:26.472781638Z",
     "start_time": "2023-12-26T10:20:36.144445731Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1 ; Batch 100 ; Train loss:  2.192326328754425\n",
      "Epoch  1 ; Batch 200 ; Train loss:  1.140281103849411\n",
      "Epoch  1 ; Batch 300 ; Train loss:  0.5629602247476577\n",
      "Epoch  1 ; Batch 400 ; Train loss:  0.3253117306530476\n",
      "Epoch  1 ; Batch 500 ; Train loss:  0.26084064081311226\n",
      "Epoch  1 ; Batch 600 ; Train loss:  0.22310261286795138\n",
      "Epoch  1 ; Batch 700 ; Train loss:  0.20458401769399642\n",
      "Epoch  1 ; Batch 800 ; Train loss:  0.18667182497680188\n",
      "Epoch  1 ; Batch 900 ; Train loss:  0.17509812459349633\n",
      "Epoch  2 ; Batch 100 ; Train loss:  0.1574059896916151\n",
      "Epoch  2 ; Batch 200 ; Train loss:  0.14297232896089554\n",
      "Epoch  2 ; Batch 300 ; Train loss:  0.13662599951028823\n",
      "Epoch  2 ; Batch 400 ; Train loss:  0.13932531051337718\n",
      "Epoch  2 ; Batch 500 ; Train loss:  0.13653416607528926\n",
      "Epoch  2 ; Batch 600 ; Train loss:  0.12195083867758512\n",
      "Epoch  2 ; Batch 700 ; Train loss:  0.13043525429442526\n",
      "Epoch  2 ; Batch 800 ; Train loss:  0.12230964431539178\n",
      "Epoch  2 ; Batch 900 ; Train loss:  0.126055430714041\n",
      "Epoch  3 ; Batch 100 ; Train loss:  0.11489749884232879\n",
      "Epoch  3 ; Batch 200 ; Train loss:  0.09785239042714239\n",
      "Epoch  3 ; Batch 300 ; Train loss:  0.1011996346153319\n",
      "Epoch  3 ; Batch 400 ; Train loss:  0.0942279090732336\n",
      "Epoch  3 ; Batch 500 ; Train loss:  0.09506598462350667\n",
      "Epoch  3 ; Batch 600 ; Train loss:  0.10678648600354791\n",
      "Epoch  3 ; Batch 700 ; Train loss:  0.09343310014344752\n",
      "Epoch  3 ; Batch 800 ; Train loss:  0.08723316358402372\n",
      "Epoch  3 ; Batch 900 ; Train loss:  0.09882817615754902\n",
      "Epoch  4 ; Batch 100 ; Train loss:  0.08309361488558352\n",
      "Epoch  4 ; Batch 200 ; Train loss:  0.08796152033610269\n",
      "Epoch  4 ; Batch 300 ; Train loss:  0.08572466634213924\n",
      "Epoch  4 ; Batch 400 ; Train loss:  0.0858914598915726\n",
      "Epoch  4 ; Batch 500 ; Train loss:  0.08620848627761006\n",
      "Epoch  4 ; Batch 600 ; Train loss:  0.07540811103768646\n",
      "Epoch  4 ; Batch 700 ; Train loss:  0.07046508956700563\n",
      "Epoch  4 ; Batch 800 ; Train loss:  0.08127433961257338\n",
      "Epoch  4 ; Batch 900 ; Train loss:  0.07455384568776935\n",
      "Epoch  5 ; Batch 100 ; Train loss:  0.07313661195803434\n",
      "Epoch  5 ; Batch 200 ; Train loss:  0.08424675984308123\n",
      "Epoch  5 ; Batch 300 ; Train loss:  0.0703810208546929\n",
      "Epoch  5 ; Batch 400 ; Train loss:  0.0668951235874556\n",
      "Epoch  5 ; Batch 500 ; Train loss:  0.07208781621418893\n",
      "Epoch  5 ; Batch 600 ; Train loss:  0.0703858788125217\n",
      "Epoch  5 ; Batch 700 ; Train loss:  0.06645470074377954\n",
      "Epoch  5 ; Batch 800 ; Train loss:  0.06953809279948472\n",
      "Epoch  5 ; Batch 900 ; Train loss:  0.0684330025780946\n",
      "Epoch  6 ; Batch 100 ; Train loss:  0.06488018595613539\n",
      "Epoch  6 ; Batch 200 ; Train loss:  0.05807275541126728\n",
      "Epoch  6 ; Batch 300 ; Train loss:  0.07296671699732542\n",
      "Epoch  6 ; Batch 400 ; Train loss:  0.057781618437729776\n",
      "Epoch  6 ; Batch 500 ; Train loss:  0.06128268990200013\n",
      "Epoch  6 ; Batch 600 ; Train loss:  0.06734407777898013\n",
      "Epoch  6 ; Batch 700 ; Train loss:  0.07459846965502948\n",
      "Epoch  6 ; Batch 800 ; Train loss:  0.06257147513562814\n",
      "Epoch  6 ; Batch 900 ; Train loss:  0.060925134799908846\n",
      "Epoch  7 ; Batch 100 ; Train loss:  0.05897672798368148\n",
      "Epoch  7 ; Batch 200 ; Train loss:  0.06143256459152326\n",
      "Epoch  7 ; Batch 300 ; Train loss:  0.061605690116994086\n",
      "Epoch  7 ; Batch 400 ; Train loss:  0.05307373521849513\n",
      "Epoch  7 ; Batch 500 ; Train loss:  0.05705104360356927\n",
      "Epoch  7 ; Batch 600 ; Train loss:  0.060383588937111196\n",
      "Epoch  7 ; Batch 700 ; Train loss:  0.0655976247228682\n",
      "Epoch  7 ; Batch 800 ; Train loss:  0.0569449459714815\n",
      "Epoch  7 ; Batch 900 ; Train loss:  0.05250963987316936\n",
      "Epoch  8 ; Batch 100 ; Train loss:  0.05591247306670993\n",
      "Epoch  8 ; Batch 200 ; Train loss:  0.05705360373482108\n",
      "Epoch  8 ; Batch 300 ; Train loss:  0.055604794998653236\n",
      "Epoch  8 ; Batch 400 ; Train loss:  0.05613448158372194\n",
      "Epoch  8 ; Batch 500 ; Train loss:  0.052126820259727535\n",
      "Epoch  8 ; Batch 600 ; Train loss:  0.05894490633625537\n",
      "Epoch  8 ; Batch 700 ; Train loss:  0.06529105155030265\n",
      "Epoch  8 ; Batch 800 ; Train loss:  0.049877545004710554\n",
      "Epoch  8 ; Batch 900 ; Train loss:  0.05551502300426364\n",
      "Epoch  9 ; Batch 100 ; Train loss:  0.0434432757855393\n",
      "Epoch  9 ; Batch 200 ; Train loss:  0.044144418600481\n",
      "Epoch  9 ; Batch 300 ; Train loss:  0.05063098189653829\n",
      "Epoch  9 ; Batch 400 ; Train loss:  0.05248220776440576\n",
      "Epoch  9 ; Batch 500 ; Train loss:  0.05265031214337796\n",
      "Epoch  9 ; Batch 600 ; Train loss:  0.044094044314697385\n",
      "Epoch  9 ; Batch 700 ; Train loss:  0.045903825096320364\n",
      "Epoch  9 ; Batch 800 ; Train loss:  0.054067806415259836\n",
      "Epoch  9 ; Batch 900 ; Train loss:  0.05394842408131808\n",
      "Epoch  10 ; Batch 100 ; Train loss:  0.041998144695535304\n",
      "Epoch  10 ; Batch 200 ; Train loss:  0.052260897519299764\n",
      "Epoch  10 ; Batch 300 ; Train loss:  0.04599951687268913\n",
      "Epoch  10 ; Batch 400 ; Train loss:  0.043983308346942064\n",
      "Epoch  10 ; Batch 500 ; Train loss:  0.050216241818852723\n",
      "Epoch  10 ; Batch 600 ; Train loss:  0.051678373265312987\n",
      "Epoch  10 ; Batch 700 ; Train loss:  0.04395015775226056\n",
      "Epoch  10 ; Batch 800 ; Train loss:  0.0482248076191172\n",
      "Epoch  10 ; Batch 900 ; Train loss:  0.052993784446734936\n",
      "Epoch  11 ; Batch 100 ; Train loss:  0.038390515600331125\n",
      "Epoch  11 ; Batch 200 ; Train loss:  0.04704067403334193\n",
      "Epoch  11 ; Batch 300 ; Train loss:  0.0407423393568024\n",
      "Epoch  11 ; Batch 400 ; Train loss:  0.048952338951639834\n",
      "Epoch  11 ; Batch 500 ; Train loss:  0.04545648362953216\n",
      "Epoch  11 ; Batch 600 ; Train loss:  0.04006640936248004\n",
      "Epoch  11 ; Batch 700 ; Train loss:  0.04484504388412461\n",
      "Epoch  11 ; Batch 800 ; Train loss:  0.04664252458605915\n",
      "Epoch  11 ; Batch 900 ; Train loss:  0.03966033265111037\n",
      "Epoch  12 ; Batch 100 ; Train loss:  0.04673210110981017\n",
      "Epoch  12 ; Batch 200 ; Train loss:  0.041739776479080316\n",
      "Epoch  12 ; Batch 300 ; Train loss:  0.04289577185176313\n",
      "Epoch  12 ; Batch 400 ; Train loss:  0.04258118123514578\n",
      "Epoch  12 ; Batch 500 ; Train loss:  0.03952741730259732\n",
      "Epoch  12 ; Batch 600 ; Train loss:  0.03662152770906687\n",
      "Epoch  12 ; Batch 700 ; Train loss:  0.04573748324182816\n",
      "Epoch  12 ; Batch 800 ; Train loss:  0.0373038381501101\n",
      "Epoch  12 ; Batch 900 ; Train loss:  0.04467008211882785\n",
      "Epoch  13 ; Batch 100 ; Train loss:  0.037226502099074424\n",
      "Epoch  13 ; Batch 200 ; Train loss:  0.04264149913797155\n",
      "Epoch  13 ; Batch 300 ; Train loss:  0.0464636810135562\n",
      "Epoch  13 ; Batch 400 ; Train loss:  0.04567217684583738\n",
      "Epoch  13 ; Batch 500 ; Train loss:  0.042507439830806105\n",
      "Epoch  13 ; Batch 600 ; Train loss:  0.03925836886279285\n",
      "Epoch  13 ; Batch 700 ; Train loss:  0.04684956712182611\n",
      "Epoch  13 ; Batch 800 ; Train loss:  0.03260706655681133\n",
      "Epoch  13 ; Batch 900 ; Train loss:  0.03425406476017088\n",
      "Epoch  14 ; Batch 100 ; Train loss:  0.04382761738728732\n",
      "Epoch  14 ; Batch 200 ; Train loss:  0.03687502179294824\n",
      "Epoch  14 ; Batch 300 ; Train loss:  0.03327639939729124\n",
      "Epoch  14 ; Batch 400 ; Train loss:  0.0424389336979948\n",
      "Epoch  14 ; Batch 500 ; Train loss:  0.03655250957002863\n",
      "Epoch  14 ; Batch 600 ; Train loss:  0.041588430312694984\n",
      "Epoch  14 ; Batch 700 ; Train loss:  0.03516455146018416\n",
      "Epoch  14 ; Batch 800 ; Train loss:  0.03967294161440805\n",
      "Epoch  14 ; Batch 900 ; Train loss:  0.03703262214665301\n",
      "Epoch  15 ; Batch 100 ; Train loss:  0.036594904955709356\n",
      "Epoch  15 ; Batch 200 ; Train loss:  0.032062382814474405\n",
      "Epoch  15 ; Batch 300 ; Train loss:  0.03408277247217484\n",
      "Epoch  15 ; Batch 400 ; Train loss:  0.03388488551136106\n",
      "Epoch  15 ; Batch 500 ; Train loss:  0.038314386155689134\n",
      "Epoch  15 ; Batch 600 ; Train loss:  0.032079160232096914\n",
      "Epoch  15 ; Batch 700 ; Train loss:  0.038903983595082535\n",
      "Epoch  15 ; Batch 800 ; Train loss:  0.03456168446922675\n",
      "Epoch  15 ; Batch 900 ; Train loss:  0.040585329616442324\n",
      "Epoch  16 ; Batch 100 ; Train loss:  0.027909133484354243\n",
      "Epoch  16 ; Batch 200 ; Train loss:  0.03965399453183636\n",
      "Epoch  16 ; Batch 300 ; Train loss:  0.035195876685902475\n",
      "Epoch  16 ; Batch 400 ; Train loss:  0.031655794555554166\n",
      "Epoch  16 ; Batch 500 ; Train loss:  0.040273790113860744\n",
      "Epoch  16 ; Batch 600 ; Train loss:  0.03606115420581773\n",
      "Epoch  16 ; Batch 700 ; Train loss:  0.03652268871082924\n",
      "Epoch  16 ; Batch 800 ; Train loss:  0.030479747519129886\n",
      "Epoch  16 ; Batch 900 ; Train loss:  0.038022692190716044\n",
      "Epoch  17 ; Batch 100 ; Train loss:  0.03687185690971091\n",
      "Epoch  17 ; Batch 200 ; Train loss:  0.03296702985069715\n",
      "Epoch  17 ; Batch 300 ; Train loss:  0.026660702535882593\n",
      "Epoch  17 ; Batch 400 ; Train loss:  0.040278119350550694\n",
      "Epoch  17 ; Batch 500 ; Train loss:  0.03046996363380458\n",
      "Epoch  17 ; Batch 600 ; Train loss:  0.03429156184545718\n",
      "Epoch  17 ; Batch 700 ; Train loss:  0.032254369257716464\n",
      "Epoch  17 ; Batch 800 ; Train loss:  0.03397786653600633\n",
      "Epoch  17 ; Batch 900 ; Train loss:  0.030132423075847327\n",
      "Epoch  18 ; Batch 100 ; Train loss:  0.0366552043822594\n",
      "Epoch  18 ; Batch 200 ; Train loss:  0.027166472661774604\n",
      "Epoch  18 ; Batch 300 ; Train loss:  0.029560214759549126\n",
      "Epoch  18 ; Batch 400 ; Train loss:  0.03173344822542276\n",
      "Epoch  18 ; Batch 500 ; Train loss:  0.027419503791606986\n",
      "Epoch  18 ; Batch 600 ; Train loss:  0.03208077120129019\n",
      "Epoch  18 ; Batch 700 ; Train loss:  0.03442948107607663\n",
      "Epoch  18 ; Batch 800 ; Train loss:  0.03732141843531281\n",
      "Epoch  18 ; Batch 900 ; Train loss:  0.03016195101168705\n",
      "Epoch  19 ; Batch 100 ; Train loss:  0.02726628006057581\n",
      "Epoch  19 ; Batch 200 ; Train loss:  0.03078203310375102\n",
      "Epoch  19 ; Batch 300 ; Train loss:  0.032308188958559185\n",
      "Epoch  19 ; Batch 400 ; Train loss:  0.0331456927745603\n",
      "Epoch  19 ; Batch 500 ; Train loss:  0.033413093506824226\n",
      "Epoch  19 ; Batch 600 ; Train loss:  0.028339125225902536\n",
      "Epoch  19 ; Batch 700 ; Train loss:  0.030218876440194435\n",
      "Epoch  19 ; Batch 800 ; Train loss:  0.03607047759520356\n",
      "Epoch  19 ; Batch 900 ; Train loss:  0.028228915387298912\n",
      "Epoch  20 ; Batch 100 ; Train loss:  0.026466053860494866\n",
      "Epoch  20 ; Batch 200 ; Train loss:  0.028706239049788564\n",
      "Epoch  20 ; Batch 300 ; Train loss:  0.03679262708639726\n",
      "Epoch  20 ; Batch 400 ; Train loss:  0.025949748005950825\n",
      "Epoch  20 ; Batch 500 ; Train loss:  0.030779967078124172\n",
      "Epoch  20 ; Batch 600 ; Train loss:  0.022973983323900028\n",
      "Epoch  20 ; Batch 700 ; Train loss:  0.029850430936785414\n",
      "Epoch  20 ; Batch 800 ; Train loss:  0.028186312226462177\n",
      "Epoch  20 ; Batch 900 ; Train loss:  0.02899315840564668\n",
      "Epoch  21 ; Batch 100 ; Train loss:  0.0255120171722956\n",
      "Epoch  21 ; Batch 200 ; Train loss:  0.0253400458896067\n",
      "Epoch  21 ; Batch 300 ; Train loss:  0.03496734812739305\n",
      "Epoch  21 ; Batch 400 ; Train loss:  0.03051768495060969\n",
      "Epoch  21 ; Batch 500 ; Train loss:  0.028831697328714653\n",
      "Epoch  21 ; Batch 600 ; Train loss:  0.026904473726172\n",
      "Epoch  21 ; Batch 700 ; Train loss:  0.027236280424986034\n",
      "Epoch  21 ; Batch 800 ; Train loss:  0.029655327495420353\n",
      "Epoch  21 ; Batch 900 ; Train loss:  0.027408416509861127\n",
      "Epoch  22 ; Batch 100 ; Train loss:  0.020905575080541895\n",
      "Epoch  22 ; Batch 200 ; Train loss:  0.023878809014568105\n",
      "Epoch  22 ; Batch 300 ; Train loss:  0.033090752239513674\n",
      "Epoch  22 ; Batch 400 ; Train loss:  0.025491072694421746\n",
      "Epoch  22 ; Batch 500 ; Train loss:  0.02424143775308039\n",
      "Epoch  22 ; Batch 600 ; Train loss:  0.029320878745638764\n",
      "Epoch  22 ; Batch 700 ; Train loss:  0.03259900826960802\n",
      "Epoch  22 ; Batch 800 ; Train loss:  0.024877214160806033\n",
      "Epoch  22 ; Batch 900 ; Train loss:  0.030700197917758486\n",
      "Epoch  23 ; Batch 100 ; Train loss:  0.028888593768351713\n",
      "Epoch  23 ; Batch 200 ; Train loss:  0.028883366680238397\n",
      "Epoch  23 ; Batch 300 ; Train loss:  0.03217490828712471\n",
      "Epoch  23 ; Batch 400 ; Train loss:  0.025962223693495615\n",
      "Epoch  23 ; Batch 500 ; Train loss:  0.026757774107391014\n",
      "Epoch  23 ; Batch 600 ; Train loss:  0.028481081457575785\n",
      "Epoch  23 ; Batch 700 ; Train loss:  0.028272795392549597\n",
      "Epoch  23 ; Batch 800 ; Train loss:  0.02414176891266834\n",
      "Epoch  23 ; Batch 900 ; Train loss:  0.02898229531245306\n",
      "Epoch  24 ; Batch 100 ; Train loss:  0.02530939539312385\n",
      "Epoch  24 ; Batch 200 ; Train loss:  0.02614770658197813\n",
      "Epoch  24 ; Batch 300 ; Train loss:  0.02555434626061469\n",
      "Epoch  24 ; Batch 400 ; Train loss:  0.024167726111481897\n",
      "Epoch  24 ; Batch 500 ; Train loss:  0.02343089522095397\n",
      "Epoch  24 ; Batch 600 ; Train loss:  0.02711570243467577\n",
      "Epoch  24 ; Batch 700 ; Train loss:  0.030063622237648815\n",
      "Epoch  24 ; Batch 800 ; Train loss:  0.0312642280786531\n",
      "Epoch  24 ; Batch 900 ; Train loss:  0.027642671183275524\n",
      "Epoch  25 ; Batch 100 ; Train loss:  0.02446624723263085\n",
      "Epoch  25 ; Batch 200 ; Train loss:  0.02138590396847576\n",
      "Epoch  25 ; Batch 300 ; Train loss:  0.030429137596511283\n",
      "Epoch  25 ; Batch 400 ; Train loss:  0.021927558729657903\n",
      "Epoch  25 ; Batch 500 ; Train loss:  0.02314354625996202\n",
      "Epoch  25 ; Batch 600 ; Train loss:  0.0222315995558165\n",
      "Epoch  25 ; Batch 700 ; Train loss:  0.024176351090427487\n",
      "Epoch  25 ; Batch 800 ; Train loss:  0.024075094904983416\n",
      "Epoch  25 ; Batch 900 ; Train loss:  0.02373542455432471\n",
      "Epoch  26 ; Batch 100 ; Train loss:  0.02167642597691156\n",
      "Epoch  26 ; Batch 200 ; Train loss:  0.028838876839436124\n",
      "Epoch  26 ; Batch 300 ; Train loss:  0.02897131963283755\n",
      "Epoch  26 ; Batch 400 ; Train loss:  0.02617910900968127\n",
      "Epoch  26 ; Batch 500 ; Train loss:  0.027162852448527702\n",
      "Epoch  26 ; Batch 600 ; Train loss:  0.023364203316159547\n",
      "Epoch  26 ; Batch 700 ; Train loss:  0.02246720248949714\n",
      "Epoch  26 ; Batch 800 ; Train loss:  0.02225377262220718\n",
      "Epoch  26 ; Batch 900 ; Train loss:  0.024809068221366033\n",
      "Epoch  27 ; Batch 100 ; Train loss:  0.016949335288954898\n",
      "Epoch  27 ; Batch 200 ; Train loss:  0.0258197146456223\n",
      "Epoch  27 ; Batch 300 ; Train loss:  0.02525185971520841\n",
      "Epoch  27 ; Batch 400 ; Train loss:  0.020934857369866222\n",
      "Epoch  27 ; Batch 500 ; Train loss:  0.020806160559877752\n",
      "Epoch  27 ; Batch 600 ; Train loss:  0.021702208402566613\n",
      "Epoch  27 ; Batch 700 ; Train loss:  0.024938411838375033\n",
      "Epoch  27 ; Batch 800 ; Train loss:  0.02360897207923699\n",
      "Epoch  27 ; Batch 900 ; Train loss:  0.027744506180752068\n",
      "Epoch  28 ; Batch 100 ; Train loss:  0.019959541182615795\n",
      "Epoch  28 ; Batch 200 ; Train loss:  0.020366163557046093\n",
      "Epoch  28 ; Batch 300 ; Train loss:  0.02339051581773674\n",
      "Epoch  28 ; Batch 400 ; Train loss:  0.024055834265891463\n",
      "Epoch  28 ; Batch 500 ; Train loss:  0.023645730764546898\n",
      "Epoch  28 ; Batch 600 ; Train loss:  0.02233578243874945\n",
      "Epoch  28 ; Batch 700 ; Train loss:  0.024132942359428852\n",
      "Epoch  28 ; Batch 800 ; Train loss:  0.020396012599230743\n",
      "Epoch  28 ; Batch 900 ; Train loss:  0.022009552791714668\n",
      "Epoch  29 ; Batch 100 ; Train loss:  0.023209954304038546\n",
      "Epoch  29 ; Batch 200 ; Train loss:  0.023623479589587076\n",
      "Epoch  29 ; Batch 300 ; Train loss:  0.02185588092368562\n",
      "Epoch  29 ; Batch 400 ; Train loss:  0.02412829043896636\n",
      "Epoch  29 ; Batch 500 ; Train loss:  0.02341607409878634\n",
      "Epoch  29 ; Batch 600 ; Train loss:  0.021910792649141513\n",
      "Epoch  29 ; Batch 700 ; Train loss:  0.024742120285518466\n",
      "Epoch  29 ; Batch 800 ; Train loss:  0.023702876038732938\n",
      "Epoch  29 ; Batch 900 ; Train loss:  0.01801439611648675\n",
      "Epoch  30 ; Batch 100 ; Train loss:  0.021721623975900002\n",
      "Epoch  30 ; Batch 200 ; Train loss:  0.020422240453772247\n",
      "Epoch  30 ; Batch 300 ; Train loss:  0.021167301259702072\n",
      "Epoch  30 ; Batch 400 ; Train loss:  0.017848951209452936\n",
      "Epoch  30 ; Batch 500 ; Train loss:  0.020780250064271966\n",
      "Epoch  30 ; Batch 600 ; Train loss:  0.021739030982134862\n",
      "Epoch  30 ; Batch 700 ; Train loss:  0.024315902640373678\n",
      "Epoch  30 ; Batch 800 ; Train loss:  0.01881986686086748\n",
      "Epoch  30 ; Batch 900 ; Train loss:  0.024218348579015583\n",
      "Finished training!\n"
     ]
    }
   ],
   "source": [
    "epochs = 30\n",
    "\n",
    "train(epochs, net)\n",
    "print(\"Finished training!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9eb50bed-db23-4105-8d3d-66d8bcdc61ef",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-20T20:17:34.238596446Z",
     "start_time": "2023-12-20T20:17:34.197665527Z"
    }
   },
   "outputs": [],
   "source": [
    "torch.save(net.state_dict(), \"./mnist_model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 28, 28])\n",
      "Predicted: 5 , True: 5\n"
     ]
    }
   ],
   "source": [
    "## testing the model by hand\n",
    "\n",
    "net.eval()\n",
    "image, label = next(iter(test_loader))\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "num = np.random.randint(batch_size)\n",
    "x = torch.unsqueeze(image[num], 0)\n",
    "print(x.shape)\n",
    "with torch.no_grad():\n",
    "    print(\"Predicted:\", torch.argmax(net(x)).item(), \", True:\", label[num].item())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-26T10:32:37.027899753Z",
     "start_time": "2023-12-26T10:32:36.984453051Z"
    }
   },
   "id": "cd4efbef29ca30b6",
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def test_model(test_dataloader, model):\n",
    "    correct_pred = 0\n",
    "    model.eval()\n",
    "\n",
    "    for images, labels in test_dataloader:\n",
    "        for i in range(len(labels)):\n",
    "            image = torch.unsqueeze(images[i], 0)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                output = model(image)\n",
    "\n",
    "            label_predicted = torch.argmax(output).item()\n",
    "            label_true = labels[i].item()\n",
    "\n",
    "            if label_predicted == label_true:\n",
    "                correct_pred += 1\n",
    "\n",
    "    total = len(test_data)\n",
    "\n",
    "    return correct_pred / total\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-26T10:32:40.126770260Z",
     "start_time": "2023-12-26T10:32:40.111538739Z"
    }
   },
   "id": "422a27e825e8fc43",
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of the model on the test dataset: 99.35000000000001%\n"
     ]
    }
   ],
   "source": [
    "accuracy = test_model(test_loader, net)\n",
    "\n",
    "print(f\"The accuracy of the model on the test dataset: {accuracy * 100}%\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-26T10:32:47.711636203Z",
     "start_time": "2023-12-26T10:32:42.938053603Z"
    }
   },
   "id": "986d51820758b0ac",
   "execution_count": 15
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
